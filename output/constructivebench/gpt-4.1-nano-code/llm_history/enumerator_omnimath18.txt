[write_program]
This problem involves a set of pairs representing edges in a graph with 2017 vertices, each edge with an associated cost. The goal appears to find the maximum possible total sum of minimal costs over edges, with given constraints about uniqueness of vertices in edges and cost calculations.

Given the constraints:
- 2017 edges, each between distinct vertices.
- For each edge, cost is the number of edges incident on either endpoint.
- The total sum is the sum over all edges of the minimum of the costs of its endpoints.

A key observation:
- If the edges form a star centered at one vertex, the costs are maximized for that center vertex (which has degree = 2016) with all others being degree 1. 
- The total sum in that case simplifies to: one node with degree 2016 contributes 2016 * 1 (since the min of 2016 and 1 is 1), and the remaining 2016 nodes (degree 1) contribute 2016 * 1 each (for their incident edges). But in a star, each of the other nodes has degree 1, connected only to the center.

Let's analyze the star:
- Center vertex: degree = 2016
- Other vertices: degree = 1 each
- Total edges = 2017, exactly one vertex connects to all others.

Cost for each edge:
- For edges connecting center to leaves: min(degree_center, degree_leaf) = min(2016, 1) = 1
- Sum over all edges: 2017 edges, each with min cost 1, total = 2017 * 1 = 2017.

This configuration is feasible and seems optimal because any other configuration would decrease the minimums in some edges, reducing total sum.

Now, considering the implementation, we can generate an explicit example with a star, verify the total sum, and output the maximum total accordingly.

```python
# Generate the maximum total by creating a star graph with center node 0
# Edges connect node 0 to all other nodes from 1 to 2016
# Cost for each edge is min(deg(center), deg(leaf)) = min(2016, 1) = 1
# Sum over all edges: number of edges * 1 = 2017

# For the purpose of demonstration, this Python code lists a representative configuration
# and computes the total sum.

# Construct edges of a star graph
edges = [(0, i) for i in range(1, 2017)]

# Compute cost for each edge: min degree of endpoints
deg_center = 2016
deg_leaf = 1
total_sum = sum(min(deg_center, deg_leaf) for _ in edges)

print(f"Constructed star with center 0; total sum: {total_sum}")
print("Example edges:", edges[:10], "...")  # show first few edges for illustration
```
